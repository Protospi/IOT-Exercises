---
title: "Predict Barbel Lifts Execution Based on IOT Devices Sensors Data"
author: "Pedro Loes"
date: "2022-09-18"
output: pdf_document
---

### Summary

This project is the final assigment Coursera Pratical Machine Learning course and was developed to build a model that predicts between 5 classes of correct and incorrect barbel lifts executions based on features retrieve by __IOT__ devices. The project consist of __6__ steps. First was data retrieving. Second, the pre processing step that consists of data wrangling and cleaning. Third was implemented a exploratory data analysis in shiny to understand features relation to the outcome response __classe__. A model was developed and the prediction were made to the test data. Finally a prediction on the test set was implemented and submitted to evaluation.

### Load Data and Descriptive analysis

The link to the [url](http://groupware.les.inf.puc-rio.br/har) of the data source is broken making impossible to retrieve a data catalog, meta information or the project design to understand for wich purpose and how the data was collected. Arbitrary decisions were taken about the project intents and data collection interpretations based on the assigment instructions.

* The `pml-training.csv` and `pml-testing` files were loaded using `data.table::fread` and transformed to `tibble` structure.
All the missing information was transformed to `NA` data type and the first column index was droped.

* Dimensions
  - Training: __19622__ observations, __158__ features and __1__ target
  - Testing: __20__ observations, __158__ features and __1__ problem id variable

### Pre Process

* Hypothesis of write failures on the __IOT__ devices were desconsidered and all the missing dat was transformed to __0__ representing no measures on the sensors for the exercise.

* Features with all observations containing missing information or features with __zero variance__ were droped because no information or improvemet to the model came from these metrics.

* The categorical date feature `cvtd_timestamp` range from __2011-11-28__ to __2011-12-05__ and represent a week training of the __6__ subjects, as well as `raw_timestamp_part_1` and `raw_timestamp_part_2`. Time in hours and minutes was also avaliable, but the exercises are executed in seconds so these features were droped as the main puspose of the project is not to understand pattern based on date and time but based on IOT exercises sensors giving the body position and movement during the exercise execution. 

* Also the features `user_name`, `num_window` and `new_window` were droped. The reason was because it's not interesting to predict a class of the exercise execution based on the subject or window but based on the data captured by the sensor at the exercise execution. By this way the sensor could help any people in the future to execute the exercise properly and possible ring a bell if the exercise execution is incorrect and the model predict it.

* Dimensions
  - Training: __19622__ observations, __144__ features and __1__ target
  - Testing: __20__ observations, __144__ features and __1__ problem id variable

### Exploratory Analysis

To compare the distribution of classes for each variable an shiny report was built in an separete html shiny report and published at shiny.io [https://loes.shinyapps.io/finalProject_shiny](https://loes.shinyapps.io/finalProject_shiny/) with a select input to subset the data on the __IOT sensor__ device to enable fast comparison between each metric and the outcome. The scale of the features were also normalized. 

```{r, echo =FALSE, message=FALSE, error=FALSE, cache=FALSE, warning=FALSE}

# Load libraries
library(shiny)
library(data.table)
library(tidyverse)
library(naniar)
library(caret)

# Load data
train <- fread("pml-training.csv", 
               na.strings = c("", "NA", "NaN", "<NA>"),
               select = 2:160) %>%  as_tibble()
test <- fread("pml-testing.csv",
              na.strings = c("", "NA", "NaN", "<NA>"),
              select = 2:160) %>%  as_tibble()

# Inspect and transform missing data to zero
misses <- miss_var_summary(train)
var_names <- misses[which(misses$pct_miss == 100), "variable"]
training <- train %>% select(-var_names$variable) %>% replace(is.na(.), 0)
testing <- test %>%  select(-var_names$variable) %>% replace(is.na(.), 0)

# Check with any variables has var 0
zero_vars <- names(which(apply(training, 2, var) == 0))
training <- training %>% select(-zero_vars)
testing <- testing %>% select(-zero_vars)

# Remove unecessary variable
data_types <- sapply(training, class) 
categorical <- names(data_types[data_types == "character"])
remove <- c(categorical[-4], "num_window", "raw_timestamp_part_1", "raw_timestamp_part_2")
training <- training %>% select(-remove)
testing <- testing %>% select(-remove)

# Pre Proces center and scale data
preProc  <- preProcess(training[-144], method = c("center", "scale"))
training <- cbind(predict(preProc, training[-144]), training["classe"])
testing <- predict(preProc, testing)

```

![](roll_belt.png)

[https://loes.shinyapps.io/finalProject_shiny/](https://loes.shinyapps.io/finalProject_shiny/)

* An example of a potencial good predictor to distinguish `classe` __A__ from the other `classe` is the __Roll Belt metric__. Just this class __E__ has records above __1.2__ treshold and these pattern could be used to correct classify as `classe` __E__ when the new observation has values these big. 

This tool was also used to spot __outliers__ and strange pattern in the data.

![](std_roll_dumb.png)

[https://loes.shinyapps.io/finalProject_shiny/](https://loes.shinyapps.io/finalProject_shiny/)

* An interesting pattern found was that a few metrics has dispersion up to __30__ standard deviation above and below the mean like the metric `stddev_roll_dumbbell`. These observations were not treated as outliers and removed because they can help to distinguish between classes.

![](gyros.png)

[https://loes.shinyapps.io/finalProject_shiny/](https://loes.shinyapps.io/finalProject_shiny/)

* Some very attipical points were spoted very far from the the distribution center of mass like `giros_forearm_z` with more then __100__ standard deviation from the mean. These observartions were manteined and will be treated by the __PCA__ techinic. Although these atipical behavior could be due to measure erros or device malfunction they were held due to the lack of information about the data extraction and the experiment design.

### Fit Models

The first step was to reduce the curse of dimensionality of the dataset projecting the predictors on a lower dimensional space. After applying principal component analysis on the `preProcess` step, the original __144__ features representing the __IOT__ sensors were reduced to __44__ features capturing up to __90%__ of the data variability.

Three model were implemented and compared considering repeated cross validations with __3__ repetitions and folds of size __10__.

* Tree model:
  - Accuracy: __0.4095242__.
  - The best model used the parameters cp = __0.01880074__
  
* Naive Bayes:
  - Accuray (gaussian): __0.2422454__
  - Accuracy (non parametric kernel): __0.5864673__. 
  
* Random Forest model:
  - Accuracy: __0.9738727__.
  - Mtry parameter used was $\sqrt{44} = 7$

### Predictions

```{r, echo = FALSE, warning=FALSE, message=FALSE}

# Load data
preds <- read_csv("predictions.csv")

# Display predictions as tables
library(knitr)
kable(preds)

```


* The final best model __Random Forest__ perform much better then the __Naive Bayes__ more then 100% better related to the model with just __1__ tree. It shows that the bootstrap of trees with random selected predictors results in a much better performance. This fact is probably because on the tree model a good predictor feature was always select on the cross validation process not giving space to other features shiny and help on the overall model.

* There's a down side for the result of the random forest. The model computational cost was at least __20__ more expensive compared to the other __2__ simpler models.

